{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "760d7be8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def compute_composite_score(df):\n",
    "    df['time_taken_norm'] = df.groupby('instance_id')['time_taken'].transform(\n",
    "        lambda x: (x - x.min()) / (x.max() - x.min() + 1e-8))\n",
    "    df['distance_gap_norm'] = df.groupby('instance_id')['distance_gap'].transform(\n",
    "        lambda x: (x - x.min()) / (x.max() - x.min() + 1e-8))\n",
    "\n",
    "    is_small = df['nr_cities'] <= 40\n",
    "    is_large = df['nr_cities'] > 40\n",
    "\n",
    "    strat = df['strategy'].str.lower()\n",
    "\n",
    "    # penalize strategies based on their characteristics\n",
    "    strat_bonus = np.where(strat == 'greedy', -0.02, \n",
    "                   np.where(strat == 'kmeans greedy', 0.0,\n",
    "                   np.where(strat == 'branch and cut', np.where(is_large, 0.2, -0.3),\n",
    "                   np.where(strat == 'ant colony', -0.1, 0.0))))\n",
    "\n",
    "    df['composite_score'] = (\n",
    "        df['normalized_cost'] * 0.65 +\n",
    "        df['time_taken_norm'] * 0.25 +\n",
    "        df['distance_gap_norm'] * 0.1 +\n",
    "        strat_bonus\n",
    "    )\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "35d80945",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300, Train Loss: 1.0049, Val Loss: 1.0349\n",
      "Epoch 2/300, Train Loss: 0.9363, Val Loss: 0.9584\n",
      "Epoch 3/300, Train Loss: 0.7790, Val Loss: 0.8810\n",
      "Epoch 4/300, Train Loss: 0.5834, Val Loss: 0.8250\n",
      "Epoch 5/300, Train Loss: 0.5665, Val Loss: 0.8141\n",
      "Epoch 6/300, Train Loss: 0.5515, Val Loss: 0.7966\n",
      "Epoch 7/300, Train Loss: 0.6648, Val Loss: 0.7868\n",
      "Epoch 8/300, Train Loss: 0.5532, Val Loss: 0.7826\n",
      "Epoch 9/300, Train Loss: 1.1890, Val Loss: 0.7729\n",
      "Epoch 10/300, Train Loss: 0.7666, Val Loss: 0.7682\n",
      "Epoch 11/300, Train Loss: 1.3765, Val Loss: 0.7686\n",
      "Epoch 12/300, Train Loss: 1.3908, Val Loss: 0.7620\n",
      "Epoch 13/300, Train Loss: 1.3711, Val Loss: 0.7596\n",
      "Epoch 14/300, Train Loss: 0.5529, Val Loss: 0.7560\n",
      "Epoch 15/300, Train Loss: 1.2254, Val Loss: 0.7578\n",
      "Epoch 16/300, Train Loss: 0.5792, Val Loss: 0.7587\n",
      "Epoch 17/300, Train Loss: 0.7327, Val Loss: 0.7563\n",
      "Epoch 18/300, Train Loss: 0.5608, Val Loss: 0.7576\n",
      "Epoch 19/300, Train Loss: 1.3858, Val Loss: 0.7570\n",
      "Epoch 20/300, Train Loss: 0.5514, Val Loss: 0.7568\n",
      "Epoch 21/300, Train Loss: 0.8151, Val Loss: 0.7545\n",
      "Epoch 22/300, Train Loss: 0.5514, Val Loss: 0.7530\n",
      "Epoch 23/300, Train Loss: 0.5518, Val Loss: 0.7538\n",
      "Epoch 24/300, Train Loss: 0.8823, Val Loss: 0.7565\n",
      "Epoch 25/300, Train Loss: 1.2856, Val Loss: 0.7535\n",
      "Epoch 26/300, Train Loss: 0.5514, Val Loss: 0.7536\n",
      "Epoch 27/300, Train Loss: 0.6932, Val Loss: 0.7528\n",
      "Epoch 28/300, Train Loss: 0.5514, Val Loss: 0.7540\n",
      "Epoch 29/300, Train Loss: 0.5514, Val Loss: 0.7525\n",
      "Epoch 30/300, Train Loss: 0.5514, Val Loss: 0.7526\n",
      "Epoch 31/300, Train Loss: 0.5514, Val Loss: 0.7529\n",
      "Epoch 32/300, Train Loss: 0.6195, Val Loss: 0.7534\n",
      "Epoch 33/300, Train Loss: 0.5514, Val Loss: 0.7537\n",
      "Epoch 34/300, Train Loss: 0.5514, Val Loss: 0.7518\n",
      "Epoch 35/300, Train Loss: 0.5514, Val Loss: 0.7530\n",
      "Epoch 36/300, Train Loss: 0.5515, Val Loss: 0.7524\n",
      "Epoch 37/300, Train Loss: 0.5514, Val Loss: 0.7526\n",
      "Epoch 38/300, Train Loss: 1.4612, Val Loss: 0.7514\n",
      "Epoch 39/300, Train Loss: 0.5514, Val Loss: 0.7509\n",
      "Epoch 40/300, Train Loss: 0.5514, Val Loss: 0.7514\n",
      "Epoch 41/300, Train Loss: 0.5514, Val Loss: 0.7519\n",
      "Epoch 42/300, Train Loss: 1.2256, Val Loss: 0.7518\n",
      "Epoch 43/300, Train Loss: 1.3371, Val Loss: 0.7517\n",
      "Epoch 44/300, Train Loss: 0.5514, Val Loss: 0.7514\n",
      "Epoch 45/300, Train Loss: 0.5514, Val Loss: 0.7516\n",
      "Epoch 46/300, Train Loss: 0.6344, Val Loss: 0.7518\n",
      "Epoch 47/300, Train Loss: 0.5652, Val Loss: 0.7526\n",
      "Epoch 48/300, Train Loss: 0.5520, Val Loss: 0.7534\n",
      "Epoch 49/300, Train Loss: 0.5514, Val Loss: 0.7526\n",
      "Epoch 50/300, Train Loss: 0.5514, Val Loss: 0.7529\n",
      "Epoch 51/300, Train Loss: 0.5515, Val Loss: 0.7529\n",
      "Epoch 52/300, Train Loss: 0.5514, Val Loss: 0.7528\n",
      "Epoch 53/300, Train Loss: 0.5514, Val Loss: 0.7529\n",
      "Epoch 54/300, Train Loss: 0.5514, Val Loss: 0.7529\n",
      "Epoch 55/300, Train Loss: 0.5514, Val Loss: 0.7529\n",
      "Epoch 56/300, Train Loss: 0.5514, Val Loss: 0.7529\n",
      "Epoch 57/300, Train Loss: 0.8782, Val Loss: 0.7528\n",
      "Epoch 58/300, Train Loss: 0.5514, Val Loss: 0.7528\n",
      "Epoch 59/300, Train Loss: 0.5514, Val Loss: 0.7528\n",
      "Epoch 60/300, Train Loss: 0.5515, Val Loss: 0.7527\n",
      "Epoch 61/300, Train Loss: 0.5514, Val Loss: 0.7527\n",
      "Epoch 62/300, Train Loss: 0.5515, Val Loss: 0.7526\n",
      "Epoch 63/300, Train Loss: 0.5514, Val Loss: 0.7525\n",
      "Epoch 64/300, Train Loss: 1.1840, Val Loss: 0.7525\n",
      "Early stopping triggered!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import sqlite3\n",
    "\n",
    "conn = sqlite3.connect('../train_mTSP.sqlite3')\n",
    "instances = pd.read_sql_query(\"SELECT * FROM instances\", conn)\n",
    "algorithms = pd.read_sql_query(\"SELECT * FROM algorithms\", conn)\n",
    "conn.close()\n",
    "\n",
    "data = pd.merge(instances, algorithms, on=\"instance_id\")\n",
    "\n",
    "# Filter out branch and cut for large instances (they tend to give bad distributions)\n",
    "data = data[~((data['strategy'] == 'branch and cut') & (data['nr_cities'] > 50))]\n",
    "\n",
    "# Compute composite score \n",
    "\n",
    "data = compute_composite_score(data)\n",
    "\n",
    "# Select best strategy per instance\n",
    "best_strategies = data.loc[data.groupby('instance_id')['composite_score'].idxmin()]\n",
    "\n",
    "# Select features and labels\n",
    "features = best_strategies[['nr_cities', 'nr_salesmen', 'average_distance', 'stddev_distance', 'density',\n",
    "                            'salesmen_ratio', 'bounding_box_area', 'aspect_ratio', 'spread',\n",
    "                            'cluster_compactness', 'mst_total_length', 'entropy_distance_matrix']]\n",
    "labels = pd.get_dummies(best_strategies['strategy'])\n",
    "labels_order = list(labels.columns)\n",
    "with open('labels_order.txt', 'w') as f:\n",
    "    for col in labels_order:\n",
    "        f.write(col + '\\n')\n",
    "\n",
    "# Normalize features\n",
    "scaler = StandardScaler()\n",
    "features = scaler.fit_transform(features)\n",
    "\n",
    "x_train = torch.tensor(features, dtype=torch.float32)\n",
    "y_train = torch.tensor(labels.values, dtype=torch.float32)\n",
    "\n",
    "\n",
    "conn = sqlite3.connect('../validation_mTSP.sqlite3')\n",
    "validation_instances = pd.read_sql_query(\"SELECT * FROM instances\", conn)\n",
    "validation_algorithms = pd.read_sql_query(\"SELECT * FROM algorithms\", conn)\n",
    "conn.close()\n",
    "\n",
    "validation_data = pd.merge(validation_instances, validation_algorithms, on=\"instance_id\")\n",
    "validation_data = validation_data[~((validation_data['strategy'] == 'branch and cut') & (validation_data['nr_cities'] > 50))]\n",
    "\n",
    "# Compute composite score for validation data\n",
    "validation_data = compute_composite_score(validation_data)\n",
    "\n",
    "# Select best strategy per validation instance\n",
    "validation_best = validation_data.loc[validation_data.groupby('instance_id')['composite_score'].idxmin()]\n",
    "\n",
    "# Select features and labels for validation\n",
    "validation_features = validation_best[['nr_cities', 'nr_salesmen', 'average_distance', 'stddev_distance', 'density',\n",
    "                                       'salesmen_ratio', 'bounding_box_area', 'aspect_ratio', 'spread',\n",
    "                                       'cluster_compactness', 'mst_total_length', 'entropy_distance_matrix']]\n",
    "validation_labels = pd.get_dummies(validation_best['strategy'])\n",
    "\n",
    "# ensure validation labels have the same columns as training labels\n",
    "for col in labels.columns:\n",
    "    if col not in validation_labels.columns:\n",
    "        validation_labels[col] = 0\n",
    "validation_labels = validation_labels[labels.columns]\n",
    "\n",
    "# Normalize validation features\n",
    "validation_features = scaler.transform(validation_features)\n",
    "\n",
    "# Ensure validation labels have the same columns as training labels\n",
    "for col in labels.columns:\n",
    "    if col not in validation_labels.columns:\n",
    "        validation_labels[col] = 0\n",
    "validation_labels = validation_labels[labels.columns]  # ensure same order\n",
    "validation_labels = validation_labels.astype(float)\n",
    "\n",
    "x_val = torch.tensor(validation_features, dtype=torch.float32)\n",
    "y_val = torch.tensor(validation_labels.values, dtype=torch.float32)\n",
    "\n",
    "# Neural Network Model \n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 64)\n",
    "        self.bn1 = nn.BatchNorm1d(64)\n",
    "        self.dropout1 = nn.Dropout(0.3)\n",
    "        self.fc2 = nn.Linear(64, 32)\n",
    "        self.bn2 = nn.BatchNorm1d(64)\n",
    "        self.dropout2 = nn.Dropout(0.3)\n",
    "        self.fc3 = nn.Linear(32, output_size)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.dropout1(x)\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.dropout2(x)\n",
    "        x = self.fc3(x)\n",
    "        return self.softmax(x)\n",
    "\n",
    "input_size = x_train.shape[1]\n",
    "output_size = y_train.shape[1]\n",
    "model = NeuralNetwork(input_size, output_size)\n",
    "\n",
    "# Calculate class weights for imbalanced classes so that the model pays more attention to underrepresented classes\n",
    "class_counts = best_strategies['strategy'].value_counts()\n",
    "class_weights = 1.0 / class_counts\n",
    "class_weights_dict = {strategy: class_weights[strategy] for strategy in class_counts.index}\n",
    "weights = torch.tensor([class_weights_dict[label] for label in labels.columns], dtype=torch.float32)\n",
    "criterion = nn.CrossEntropyLoss(weight=weights)\n",
    "# criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-5)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=50, gamma=0.1)\n",
    "\n",
    "best_val_loss = float('inf')\n",
    "patience = 25  # number of epochs to wait for improvement\n",
    "trigger_times = 0\n",
    "\n",
    "# Train\n",
    "epochs = 300\n",
    "batch_size = 32\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    permutation = torch.randperm(x_train.size(0))\n",
    "    for i in range(0, x_train.size(0), batch_size):\n",
    "        indices = permutation[i:i+batch_size]\n",
    "        batch_X, batch_y = x_train[indices], y_train[indices]\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(batch_X)\n",
    "        loss = criterion(outputs, torch.argmax(batch_y, dim=1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_outputs = model(x_val)\n",
    "        val_loss = criterion(val_outputs, torch.argmax(y_val, dim=1))\n",
    "    print(f\"Epoch {epoch+1}/{epochs}, Train Loss: {loss.item():.4f}, Val Loss: {val_loss.item():.4f}\")\n",
    "\n",
    "    # Stop if no improvement in validation loss\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        trigger_times = 0\n",
    "        torch.save(model.state_dict(), 'models/best_model.pth')  \n",
    "    else:\n",
    "        trigger_times += 1\n",
    "        if trigger_times >= patience:\n",
    "            print(\"Early stopping triggered!\")\n",
    "            break\n",
    "\n",
    "    scheduler.step()\n",
    "\n",
    "# torch.save(model.state_dict(), 'algorithm_selector_model2.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "1978dc28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "strategy\n",
      "Branch and Cut    661\n",
      "Greedy            367\n",
      "KMeans-Greedy      59\n",
      "Ant Colony          2\n",
      "Name: count, dtype: int64\n",
      "1    647\n",
      "2    262\n",
      "3    171\n",
      "0      9\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(best_strategies['strategy'].value_counts())\n",
    "\n",
    "with torch.no_grad():\n",
    "    train_outputs = model(x_train)\n",
    "    train_predictions = torch.argmax(train_outputs, dim=1)\n",
    "    print(pd.Series(train_predictions.numpy()).value_counts())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
