{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "35d80945",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300, Train Loss: 0.9218, Val Loss: 1.3603\n",
      "Epoch 2/300, Train Loss: 1.3129, Val Loss: 1.2829\n",
      "Epoch 3/300, Train Loss: 0.7552, Val Loss: 1.1531\n",
      "Epoch 4/300, Train Loss: 0.7451, Val Loss: 1.0685\n",
      "Epoch 5/300, Train Loss: 0.7448, Val Loss: 1.0079\n",
      "Epoch 6/300, Train Loss: 0.7444, Val Loss: 0.9668\n",
      "Epoch 7/300, Train Loss: 0.7516, Val Loss: 0.9507\n",
      "Epoch 8/300, Train Loss: 0.7480, Val Loss: 0.9343\n",
      "Epoch 9/300, Train Loss: 0.7817, Val Loss: 0.9303\n",
      "Epoch 10/300, Train Loss: 0.7549, Val Loss: 0.9207\n",
      "Epoch 11/300, Train Loss: 1.5837, Val Loss: 0.9140\n",
      "Epoch 12/300, Train Loss: 0.7437, Val Loss: 0.9172\n",
      "Epoch 13/300, Train Loss: 0.7840, Val Loss: 0.9139\n",
      "Epoch 14/300, Train Loss: 1.7358, Val Loss: 0.9111\n",
      "Epoch 15/300, Train Loss: 0.7439, Val Loss: 0.9095\n",
      "Epoch 16/300, Train Loss: 0.7578, Val Loss: 0.9091\n",
      "Epoch 17/300, Train Loss: 0.7774, Val Loss: 0.9098\n",
      "Epoch 18/300, Train Loss: 0.7451, Val Loss: 0.9097\n",
      "Epoch 19/300, Train Loss: 0.7437, Val Loss: 0.9123\n",
      "Epoch 20/300, Train Loss: 0.7508, Val Loss: 0.9066\n",
      "Epoch 21/300, Train Loss: 0.8303, Val Loss: 0.9081\n",
      "Epoch 22/300, Train Loss: 0.7437, Val Loss: 0.9080\n",
      "Epoch 23/300, Train Loss: 1.2020, Val Loss: 0.9080\n",
      "Epoch 24/300, Train Loss: 0.7461, Val Loss: 0.9122\n",
      "Epoch 25/300, Train Loss: 0.8536, Val Loss: 0.9082\n",
      "Epoch 26/300, Train Loss: 0.7437, Val Loss: 0.9076\n",
      "Epoch 27/300, Train Loss: 0.7437, Val Loss: 0.9085\n",
      "Epoch 28/300, Train Loss: 0.7591, Val Loss: 0.9070\n",
      "Epoch 29/300, Train Loss: 1.1022, Val Loss: 0.9065\n",
      "Epoch 30/300, Train Loss: 1.6798, Val Loss: 0.9063\n",
      "Epoch 31/300, Train Loss: 0.7443, Val Loss: 0.9076\n",
      "Epoch 32/300, Train Loss: 0.7437, Val Loss: 0.9098\n",
      "Epoch 33/300, Train Loss: 1.5944, Val Loss: 0.9103\n",
      "Epoch 34/300, Train Loss: 1.1134, Val Loss: 0.9081\n",
      "Epoch 35/300, Train Loss: 0.9181, Val Loss: 0.9081\n",
      "Epoch 36/300, Train Loss: 0.7451, Val Loss: 0.9099\n",
      "Epoch 37/300, Train Loss: 1.0265, Val Loss: 0.9091\n",
      "Epoch 38/300, Train Loss: 0.9228, Val Loss: 0.9121\n",
      "Epoch 39/300, Train Loss: 1.7419, Val Loss: 0.9181\n",
      "Epoch 40/300, Train Loss: 0.7437, Val Loss: 0.9109\n",
      "Epoch 41/300, Train Loss: 0.7437, Val Loss: 0.9064\n",
      "Epoch 42/300, Train Loss: 0.7465, Val Loss: 0.9050\n",
      "Epoch 43/300, Train Loss: 0.7450, Val Loss: 0.9042\n",
      "Epoch 44/300, Train Loss: 1.3752, Val Loss: 0.9043\n",
      "Epoch 45/300, Train Loss: 0.7438, Val Loss: 0.9089\n",
      "Epoch 46/300, Train Loss: 0.7507, Val Loss: 0.9054\n",
      "Epoch 47/300, Train Loss: 0.7438, Val Loss: 0.9046\n",
      "Epoch 48/300, Train Loss: 0.7459, Val Loss: 0.9044\n",
      "Epoch 49/300, Train Loss: 0.7490, Val Loss: 0.9037\n",
      "Epoch 50/300, Train Loss: 0.7437, Val Loss: 0.9040\n",
      "Epoch 51/300, Train Loss: 1.3656, Val Loss: 0.9040\n",
      "Epoch 52/300, Train Loss: 1.2262, Val Loss: 0.9039\n",
      "Epoch 53/300, Train Loss: 0.7437, Val Loss: 0.9041\n",
      "Epoch 54/300, Train Loss: 1.2789, Val Loss: 0.9041\n",
      "Epoch 55/300, Train Loss: 0.7439, Val Loss: 0.9040\n",
      "Epoch 56/300, Train Loss: 0.7437, Val Loss: 0.9040\n",
      "Epoch 57/300, Train Loss: 0.7441, Val Loss: 0.9040\n",
      "Epoch 58/300, Train Loss: 1.6991, Val Loss: 0.9040\n",
      "Epoch 59/300, Train Loss: 0.7463, Val Loss: 0.9040\n",
      "Epoch 60/300, Train Loss: 1.5811, Val Loss: 0.9041\n",
      "Epoch 61/300, Train Loss: 0.7437, Val Loss: 0.9043\n",
      "Epoch 62/300, Train Loss: 0.7768, Val Loss: 0.9043\n",
      "Epoch 63/300, Train Loss: 0.7443, Val Loss: 0.9043\n",
      "Epoch 64/300, Train Loss: 1.1875, Val Loss: 0.9044\n",
      "Epoch 65/300, Train Loss: 0.7437, Val Loss: 0.9048\n",
      "Epoch 66/300, Train Loss: 0.7437, Val Loss: 0.9046\n",
      "Epoch 67/300, Train Loss: 1.6672, Val Loss: 0.9045\n",
      "Epoch 68/300, Train Loss: 0.9142, Val Loss: 0.9046\n",
      "Epoch 69/300, Train Loss: 1.4290, Val Loss: 0.9047\n",
      "Epoch 70/300, Train Loss: 1.6699, Val Loss: 0.9050\n",
      "Epoch 71/300, Train Loss: 0.7518, Val Loss: 0.9050\n",
      "Epoch 72/300, Train Loss: 0.8732, Val Loss: 0.9050\n",
      "Epoch 73/300, Train Loss: 0.7437, Val Loss: 0.9050\n",
      "Epoch 74/300, Train Loss: 1.1342, Val Loss: 0.9049\n",
      "Early stopping triggered!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sqlite3\n",
    "\n",
    "conn = sqlite3.connect('../train_mTSP.sqlite3')\n",
    "instances = pd.read_sql_query(\"SELECT * FROM instances\", conn)\n",
    "algorithms = pd.read_sql_query(\"SELECT * FROM algorithms\", conn)\n",
    "conn.close()\n",
    "\n",
    "data = pd.merge(instances, algorithms, on=\"instance_id\")\n",
    "\n",
    "# Filter out branch and cut for large instances (they tend to give bad distributions)\n",
    "data = data[~((data['strategy'] == 'branch and cut') & (data['nr_cities'] > 50))]\n",
    "\n",
    "# Compute composite score \n",
    "data['time_taken_norm'] = data.groupby('instance_id')['time_taken'].transform(\n",
    "    lambda x: (x - x.min()) / (x.max() - x.min() + 1e-8)\n",
    ")\n",
    "data['distance_gap_norm'] = data.groupby('instance_id')['distance_gap'].transform(\n",
    "    lambda x: (x - x.min()) / (x.max() - x.min() + 1e-8)\n",
    ")\n",
    "\n",
    "is_small = data['nr_cities'] <= 40\n",
    "is_bnc = data['strategy'].str.lower().str.contains('branch and cut')\n",
    "is_ant = data['strategy'].str.lower().str.contains('ant colony')\n",
    "\n",
    "data['composite_score'] = (\n",
    "    data['normalized_cost'] * np.where(is_small, 0.9, 0.6) +\n",
    "    data['time_taken_norm'] * np.where(is_small & is_bnc, 0, np.where(is_small, 0.1, 0.2)) +\n",
    "    data['distance_gap_norm'] * 0.05 +\n",
    "    np.where(is_ant, -0.05, 0) \n",
    ")\n",
    "\n",
    "# Select best strategy per instance\n",
    "best_strategies = data.loc[data.groupby('instance_id')['composite_score'].idxmin()]\n",
    "\n",
    "# Select features and labels\n",
    "features = best_strategies[['nr_cities', 'nr_salesmen', 'average_distance', 'stddev_distance', 'density',\n",
    "                            'salesmen_ratio', 'bounding_box_area', 'aspect_ratio', 'spread',\n",
    "                            'cluster_compactness', 'mst_total_length', 'entropy_distance_matrix']]\n",
    "labels = pd.get_dummies(best_strategies['strategy'])\n",
    "labels_order = list(labels.columns)\n",
    "with open('labels_order.txt', 'w') as f:\n",
    "    for col in labels_order:\n",
    "        f.write(col + '\\n')\n",
    "\n",
    "# Normalize features\n",
    "scaler = StandardScaler()\n",
    "features = scaler.fit_transform(features)\n",
    "\n",
    "x_train = torch.tensor(features, dtype=torch.float32)\n",
    "y_train = torch.tensor(labels.values, dtype=torch.float32)\n",
    "\n",
    "\n",
    "conn = sqlite3.connect('../validation_mTSP.sqlite3')\n",
    "validation_instances = pd.read_sql_query(\"SELECT * FROM instances\", conn)\n",
    "validation_algorithms = pd.read_sql_query(\"SELECT * FROM algorithms\", conn)\n",
    "conn.close()\n",
    "\n",
    "validation_data = pd.merge(validation_instances, validation_algorithms, on=\"instance_id\")\n",
    "validation_data = validation_data[~((validation_data['strategy'] == 'branch and cut') & (validation_data['nr_cities'] > 50))]\n",
    "\n",
    "# Compute composite score for validation data\n",
    "validation_data['time_taken_norm'] = validation_data.groupby('instance_id')['time_taken'].transform(\n",
    "    lambda x: (x - x.min()) / (x.max() - x.min() + 1e-8)\n",
    ")\n",
    "validation_data['distance_gap_norm'] = validation_data.groupby('instance_id')['distance_gap'].transform(\n",
    "    lambda x: (x - x.min()) / (x.max() - x.min() + 1e-8)\n",
    ")\n",
    "\n",
    "is_small_val = validation_data['nr_cities'] <= 40\n",
    "is_bnc_val = validation_data['strategy'].str.lower().str.contains('branch and cut')\n",
    "is_ant_val = validation_data['strategy'].str.lower().str.contains('ant colony')\n",
    "\n",
    "validation_data['composite_score'] = (\n",
    "    validation_data['normalized_cost'] * np.where(is_small_val, 0.9, 0.6) +\n",
    "    validation_data['time_taken_norm'] * np.where(is_small_val & is_bnc_val, 0, np.where(is_small_val, 0.1, 0.2)) +\n",
    "    validation_data['distance_gap_norm'] * 0.05 +\n",
    "    np.where(is_ant_val, -0.05, 0)\n",
    ")\n",
    "\n",
    "# Select best strategy per validation instance\n",
    "validation_best = validation_data.loc[validation_data.groupby('instance_id')['composite_score'].idxmin()]\n",
    "\n",
    "# Select features and labels for validation\n",
    "validation_features = validation_best[['nr_cities', 'nr_salesmen', 'average_distance', 'stddev_distance', 'density',\n",
    "                                       'salesmen_ratio', 'bounding_box_area', 'aspect_ratio', 'spread',\n",
    "                                       'cluster_compactness', 'mst_total_length', 'entropy_distance_matrix']]\n",
    "validation_labels = pd.get_dummies(validation_best['strategy'])\n",
    "\n",
    "# ensure validation labels have the same columns as training labels\n",
    "for col in labels.columns:\n",
    "    if col not in validation_labels.columns:\n",
    "        validation_labels[col] = 0\n",
    "validation_labels = validation_labels[labels.columns]\n",
    "\n",
    "# Normalize validation features\n",
    "validation_features = scaler.transform(validation_features)\n",
    "\n",
    "# Ensure validation labels have the same columns as training labels\n",
    "for col in labels.columns:\n",
    "    if col not in validation_labels.columns:\n",
    "        validation_labels[col] = 0\n",
    "validation_labels = validation_labels[labels.columns]  # ensure same order\n",
    "validation_labels = validation_labels.astype(float)\n",
    "\n",
    "x_val = torch.tensor(validation_features, dtype=torch.float32)\n",
    "y_val = torch.tensor(validation_labels.values, dtype=torch.float32)\n",
    "\n",
    "# Neural Network Model \n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 64)\n",
    "        self.bn1 = nn.BatchNorm1d(64)\n",
    "        self.dropout1 = nn.Dropout(0.3)\n",
    "        self.fc2 = nn.Linear(64, 32)\n",
    "        self.bn2 = nn.BatchNorm1d(64)\n",
    "        self.dropout2 = nn.Dropout(0.3)\n",
    "        self.fc3 = nn.Linear(32, output_size)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.dropout1(x)\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.dropout2(x)\n",
    "        x = self.fc3(x)\n",
    "        return self.softmax(x)\n",
    "\n",
    "input_size = x_train.shape[1]\n",
    "output_size = y_train.shape[1]\n",
    "model = NeuralNetwork(input_size, output_size)\n",
    "\n",
    "# Calculate class weights for imbalanced classes so that the model pays more attention to underrepresented classes\n",
    "class_counts = best_strategies['strategy'].value_counts()\n",
    "class_weights = 1.0 / class_counts\n",
    "class_weights_dict = {strategy: class_weights[strategy] for strategy in class_counts.index}\n",
    "weights = torch.tensor([class_weights_dict[label] for label in labels.columns], dtype=torch.float32)\n",
    "criterion = nn.CrossEntropyLoss(weight=weights)\n",
    "# criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-5)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=50, gamma=0.1)\n",
    "\n",
    "best_val_loss = float('inf')\n",
    "patience = 25  # number of epochs to wait for improvement\n",
    "trigger_times = 0\n",
    "\n",
    "# Train\n",
    "epochs = 300\n",
    "batch_size = 32\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    permutation = torch.randperm(x_train.size(0))\n",
    "    for i in range(0, x_train.size(0), batch_size):\n",
    "        indices = permutation[i:i+batch_size]\n",
    "        batch_X, batch_y = x_train[indices], y_train[indices]\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(batch_X)\n",
    "        loss = criterion(outputs, torch.argmax(batch_y, dim=1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_outputs = model(x_val)\n",
    "        val_loss = criterion(val_outputs, torch.argmax(y_val, dim=1))\n",
    "    print(f\"Epoch {epoch+1}/{epochs}, Train Loss: {loss.item():.4f}, Val Loss: {val_loss.item():.4f}\")\n",
    "\n",
    "    # Stop if no improvement in validation loss\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        trigger_times = 0\n",
    "        torch.save(model.state_dict(), 'models/best_model.pth')  \n",
    "    else:\n",
    "        trigger_times += 1\n",
    "        if trigger_times >= patience:\n",
    "            print(\"Early stopping triggered!\")\n",
    "            break\n",
    "\n",
    "    scheduler.step()\n",
    "\n",
    "# torch.save(model.state_dict(), 'algorithm_selector_model2.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "1978dc28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "strategy\n",
      "Branch and Cut    661\n",
      "Greedy            367\n",
      "KMeans-Greedy      59\n",
      "Ant Colony          2\n",
      "Name: count, dtype: int64\n",
      "1    647\n",
      "2    262\n",
      "3    171\n",
      "0      9\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(best_strategies['strategy'].value_counts())\n",
    "\n",
    "with torch.no_grad():\n",
    "    train_outputs = model(x_train)\n",
    "    train_predictions = torch.argmax(train_outputs, dim=1)\n",
    "    print(pd.Series(train_predictions.numpy()).value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "6c95be83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     instance_id predicted_algorithm\n",
      "0              1      branch and cut\n",
      "1              2      branch and cut\n",
      "2              3      branch and cut\n",
      "3              4      branch and cut\n",
      "4              5      branch and cut\n",
      "..           ...                 ...\n",
      "96            97              greedy\n",
      "97            98              greedy\n",
      "98            99              greedy\n",
      "99           100              greedy\n",
      "100          101              greedy\n",
      "\n",
      "[101 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pandas as pd\n",
    "import sqlite3\n",
    "\n",
    "input_size = 12\n",
    "output_size = 4\n",
    "model = NeuralNetwork(input_size, output_size)\n",
    "model.load_state_dict(torch.load('models/best_model.pth'))\n",
    "model.eval()\n",
    "\n",
    "with open('labels_order.txt') as f:\n",
    "    label_order = [line.strip() for line in f]\n",
    "\n",
    "rename_dict = {\n",
    "    'Ant Colony': 'ant colony optimization',\n",
    "    'Branch and Cut': 'branch and cut',\n",
    "    'Greedy': 'greedy',\n",
    "    'KMeans-Greedy': 'kmeans greedy'\n",
    "}\n",
    "algorithm_mapping = {i: rename_dict[label] for i, label in enumerate(label_order)}\n",
    "\n",
    "conn = sqlite3.connect('../test_mTSP.sqlite3')\n",
    "test_instances = pd.read_sql_query(\"SELECT * FROM instances\", conn)\n",
    "conn.close()\n",
    "\n",
    "test_features = test_instances[['nr_cities', 'nr_salesmen', 'average_distance', 'stddev_distance', 'density',\n",
    "                                'salesmen_ratio', 'bounding_box_area', 'aspect_ratio', 'spread',\n",
    "                                'cluster_compactness', 'mst_total_length', 'entropy_distance_matrix']]\n",
    "\n",
    "test_features = scaler.transform(test_features)\n",
    "\n",
    "x_test = torch.tensor(test_features, dtype=torch.float32)\n",
    "\n",
    "with torch.no_grad():\n",
    "    predictions = model(x_test)\n",
    "    predicted_classes = torch.argmax(predictions, dim=1)\n",
    "\n",
    "predicted_algorithms = [algorithm_mapping[class_idx.item()] for class_idx in predicted_classes]\n",
    "\n",
    "test_instances['predicted_algorithm'] = predicted_algorithms\n",
    "print(test_instances[['instance_id', 'predicted_algorithm']])\n",
    "test_instances[['instance_id', 'predicted_algorithm']].to_csv('predicted_algorithms.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
